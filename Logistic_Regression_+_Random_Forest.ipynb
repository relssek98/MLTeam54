{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OksS3dvYQWg3",
        "YK8YHAtzbo4c",
        "4mWT90Wl2SH7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/relssek98/MLTeam54/blob/main/Logistic_Regression_%2B_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "B0unmdbGQE9B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eluS2xjAP_Mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4f41b2-ffad-4bf7-f7e4-4b44bcc9922a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "\n",
        "#added\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Open Data"
      ],
      "metadata": {
        "id": "OksS3dvYQWg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(r\"/content/drive/MyDrive/Machine Learning/Data/SDoutput1.csv\")\n",
        "from ast import literal_eval\n",
        "data['clean'] = data['clean'].apply(literal_eval)\n",
        "data.loc[data[\"label\"]=='suicide', \"label\"] = 1.0\n",
        "data.loc[data[\"label\"]=='non-suicide', \"label\"] = 0.0\n",
        "print(type(data[\"label\"][0]))\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "890HFDDLRVPv",
        "outputId": "fa666787-369f-470f-e59e-3e237b17b3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'float'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                              clean label\n",
              "0           0  [ex, wife, threaten, suiciderec, left, wife, g...   1.0\n",
              "1           1  [weird, get, affect, compliment, come, someon,...   0.0\n",
              "2           2  [final, 2020, almost, never, hear, 2020, ha, b...   0.0\n",
              "3           3                  [need, helpjust, help, cri, hard]   1.0\n",
              "4           4  [losthello, name, adam, 16, struggl, year, afr...   1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c02cefe-52da-4e6e-b566-e56b0da4be77\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>clean</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[ex, wife, threaten, suiciderec, left, wife, g...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[weird, get, affect, compliment, come, someon,...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[final, 2020, almost, never, hear, 2020, ha, b...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[need, helpjust, help, cri, hard]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[losthello, name, adam, 16, struggl, year, afr...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c02cefe-52da-4e6e-b566-e56b0da4be77')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c02cefe-52da-4e6e-b566-e56b0da4be77 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c02cefe-52da-4e6e-b566-e56b0da4be77');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "m1nnd067nX_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "unjRvO3-nX_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d739f50-d11c-48b7-8934-229f8bfdcc7d",
        "id": "-P15HjKOnX_L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "103569\n",
            "  (0, 37423)\t2\n",
            "  (0, 91283)\t1\n",
            "  (0, 55814)\t2\n",
            "  (0, 59402)\t1\n",
            "  (0, 87613)\t1\n",
            "  (0, 68970)\t1\n",
            "  (0, 67253)\t1\n",
            "  (0, 90839)\t1\n",
            "  (0, 73080)\t1\n",
            "  (0, 84052)\t1\n",
            "  (0, 28500)\t1\n",
            "  (0, 57634)\t3\n",
            "  (0, 36850)\t1\n",
            "  (0, 28155)\t1\n",
            "  (0, 25617)\t3\n",
            "  (0, 17120)\t1\n",
            "  (0, 80576)\t1\n",
            "  (0, 28034)\t1\n",
            "  (0, 3261)\t1\n",
            "  (0, 58498)\t1\n",
            "  (0, 46009)\t1\n",
            "  (0, 96826)\t1\n",
            "  (0, 23385)\t1\n",
            "  (0, 100224)\t1\n",
            "  (0, 69333)\t1\n",
            "  :\t:\n",
            "  (99963, 12837)\t1\n",
            "  (99963, 39120)\t1\n",
            "  (99964, 36467)\t1\n",
            "  (99964, 77361)\t1\n",
            "  (99964, 101712)\t1\n",
            "  (99965, 40181)\t1\n",
            "  (99965, 71483)\t1\n",
            "  (99965, 13154)\t1\n",
            "  (99965, 58244)\t1\n",
            "  (99965, 67475)\t1\n",
            "  (99965, 80262)\t1\n",
            "  (99965, 26327)\t1\n",
            "  (99965, 79429)\t1\n",
            "  (99966, 90784)\t2\n",
            "  (99966, 44226)\t1\n",
            "  (99966, 75690)\t1\n",
            "  (99966, 83023)\t1\n",
            "  (99966, 47670)\t1\n",
            "  (99966, 101160)\t1\n",
            "  (99966, 81453)\t1\n",
            "  (99966, 73184)\t1\n",
            "  (99966, 17586)\t1\n",
            "  (99966, 45943)\t1\n",
            "  (99966, 13091)\t1\n",
            "  (99966, 39405)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "WSqx1Ht4nX_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "yOGc0JfxnX_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "hghoVXUFnX_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "U5tgvnArnX_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def randomForest(x_train, y_train, x_test, y_test): \n",
        "  classif = RandomForestClassifier(n_estimators= 50)\n",
        "  classif.fit(x_train, y_train)\n",
        "  y_predict = classif.predict(x_test)\n",
        "  print(metrics.accuracy_score(y_test, y_predict))\n",
        "\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FQHBib27nX_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomForest(x_train_bow, y_train, X_te_bow, y_test)"
      ],
      "metadata": {
        "id": "xl2YcKOhFBcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522aee99-a165-4e65-b86b-d7c7c7fc9e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8895340985820391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "8QSetH9KnX_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673b05b1-89ee-472b-e8fd-6d0fc5ff9403",
        "id": "3MMEllScnX_M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "NUjKZQMnnX_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc1a001-e7dc-4ee7-def8-2230fb8a1298",
        "id": "qC0Lr5zRnX_M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "AyJ1HGConX_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "DCmFn8tjnX_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "IAoIT6wjnX_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "Ioq5ttdUnX_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "9GLQfPDlnX_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "JYV6MAK5nX_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4951064a-f551-435e-c770-047e8fe116c2",
        "id": "MvNjPldxnX_N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "102618\n",
            "  (0, 30878)\t1\n",
            "  (0, 13904)\t1\n",
            "  (0, 97957)\t2\n",
            "  (0, 14959)\t2\n",
            "  (0, 73860)\t1\n",
            "  (0, 70824)\t1\n",
            "  (0, 65344)\t1\n",
            "  (0, 40638)\t1\n",
            "  (0, 26809)\t2\n",
            "  (0, 83614)\t1\n",
            "  (0, 53741)\t2\n",
            "  (0, 98826)\t1\n",
            "  (0, 15878)\t1\n",
            "  (0, 20412)\t1\n",
            "  (1, 53741)\t1\n",
            "  (1, 86724)\t1\n",
            "  (1, 28746)\t1\n",
            "  (1, 24856)\t1\n",
            "  (1, 91242)\t1\n",
            "  (1, 27518)\t3\n",
            "  (1, 29853)\t1\n",
            "  (1, 66411)\t1\n",
            "  (1, 25660)\t2\n",
            "  (1, 36108)\t1\n",
            "  (1, 44549)\t2\n",
            "  :\t:\n",
            "  (99966, 21522)\t1\n",
            "  (99966, 42880)\t1\n",
            "  (99966, 94982)\t1\n",
            "  (99966, 33619)\t1\n",
            "  (99966, 92244)\t2\n",
            "  (99966, 16578)\t1\n",
            "  (99966, 85501)\t1\n",
            "  (99966, 55581)\t1\n",
            "  (99966, 71912)\t1\n",
            "  (99966, 47718)\t2\n",
            "  (99966, 92745)\t2\n",
            "  (99966, 52095)\t1\n",
            "  (99966, 17755)\t3\n",
            "  (99966, 63742)\t2\n",
            "  (99966, 17061)\t1\n",
            "  (99966, 39061)\t1\n",
            "  (99966, 46206)\t1\n",
            "  (99966, 35213)\t1\n",
            "  (99966, 3022)\t2\n",
            "  (99966, 78898)\t1\n",
            "  (99966, 79945)\t1\n",
            "  (99966, 4074)\t1\n",
            "  (99966, 75928)\t1\n",
            "  (99966, 74876)\t1\n",
            "  (99966, 35967)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "63ycHaY3nX_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "fisi96n5nX_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "3K0afpOgnX_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "OwDIZgGgnX_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-JDLtOtHnX_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "q8uVljEjnX_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47044c5-170a-4d79-b373-e79cda87fbbc",
        "id": "NLRfU-bLnX_N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "oiryXtqEnX_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b0dad0-7abe-482f-80ce-2e3388964bd6",
        "id": "ATnYD0aTnX_O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "q7EfHJoAnX_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "ogVe2xmWnX_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "EkFAEfnQnX_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "X2_wPouYnX_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "_SWvuzT-nX_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "vYcQmIa3nX_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4146ac4-c5ec-4bf7-e327-379f1fb13ff7",
        "id": "IqX9Xfr2nX_O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "98698\n",
            "  (0, 38100)\t1\n",
            "  (0, 45406)\t1\n",
            "  (0, 26160)\t1\n",
            "  (0, 79959)\t1\n",
            "  (1, 59414)\t1\n",
            "  (1, 79700)\t1\n",
            "  (1, 84182)\t1\n",
            "  (1, 87760)\t1\n",
            "  (1, 71137)\t1\n",
            "  (1, 12874)\t1\n",
            "  (1, 20700)\t1\n",
            "  (1, 73084)\t1\n",
            "  (1, 80169)\t1\n",
            "  (1, 35724)\t1\n",
            "  (1, 96866)\t1\n",
            "  (1, 68329)\t1\n",
            "  (1, 85078)\t1\n",
            "  (1, 55891)\t1\n",
            "  (2, 51421)\t1\n",
            "  (2, 51023)\t1\n",
            "  (2, 51846)\t2\n",
            "  (2, 40131)\t2\n",
            "  (2, 13142)\t1\n",
            "  (2, 38945)\t1\n",
            "  (2, 69107)\t1\n",
            "  :\t:\n",
            "  (99966, 60324)\t1\n",
            "  (99966, 94534)\t1\n",
            "  (99966, 55634)\t1\n",
            "  (99966, 35186)\t1\n",
            "  (99966, 74220)\t1\n",
            "  (99966, 59372)\t1\n",
            "  (99966, 69524)\t2\n",
            "  (99966, 93912)\t1\n",
            "  (99966, 14246)\t1\n",
            "  (99966, 96953)\t1\n",
            "  (99966, 70588)\t3\n",
            "  (99966, 45893)\t1\n",
            "  (99966, 19452)\t1\n",
            "  (99966, 66603)\t1\n",
            "  (99966, 69466)\t1\n",
            "  (99966, 39250)\t1\n",
            "  (99966, 84027)\t1\n",
            "  (99966, 93429)\t1\n",
            "  (99966, 73642)\t1\n",
            "  (99966, 39417)\t1\n",
            "  (99966, 42154)\t1\n",
            "  (99966, 27412)\t1\n",
            "  (99966, 34805)\t1\n",
            "  (99966, 47027)\t1\n",
            "  (99966, 71663)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "1o3y_X85nX_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "cQXRFbyZnX_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "se_6444RnX_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "IFeIiREXnX_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "83ZhDy_FnX_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "Qlw5n3YHnX_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c08aa10-43e5-424c-88b7-ca1801c78ea6",
        "id": "B4jY9G3CnX_P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "jX9dfEAKnX_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bb756b-a789-4a88-852e-b3a531b7c2f0",
        "id": "qAwwVkbCnX_P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "z_tkfFabnX_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "A7H8gx8xnX_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "DmxetUQCnX_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "YUKWfZG4nX_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "GmjcZyF6nX_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "g3z2I2hknX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c3cc45-3fd8-4179-f2e9-0a98a82c44a5",
        "id": "-U2Cvd22nX_Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "102793\n",
            "  (0, 17692)\t2\n",
            "  (0, 39656)\t3\n",
            "  (0, 29749)\t2\n",
            "  (0, 63000)\t1\n",
            "  (0, 65039)\t1\n",
            "  (0, 84206)\t1\n",
            "  (0, 61126)\t2\n",
            "  (0, 71887)\t2\n",
            "  (0, 91645)\t2\n",
            "  (0, 42945)\t3\n",
            "  (0, 44154)\t3\n",
            "  (0, 30633)\t7\n",
            "  (0, 68941)\t3\n",
            "  (0, 36824)\t3\n",
            "  (0, 90101)\t6\n",
            "  (0, 54206)\t2\n",
            "  (0, 74557)\t2\n",
            "  (0, 80020)\t3\n",
            "  (0, 83017)\t4\n",
            "  (0, 57137)\t3\n",
            "  (0, 25196)\t3\n",
            "  (0, 58875)\t1\n",
            "  (0, 66847)\t1\n",
            "  (0, 69983)\t1\n",
            "  (0, 55527)\t2\n",
            "  :\t:\n",
            "  (99966, 88588)\t1\n",
            "  (99966, 77310)\t2\n",
            "  (99966, 53177)\t1\n",
            "  (99966, 34222)\t1\n",
            "  (99966, 81179)\t1\n",
            "  (99966, 18354)\t1\n",
            "  (99966, 18351)\t1\n",
            "  (99966, 41945)\t1\n",
            "  (99966, 70071)\t1\n",
            "  (99966, 81388)\t3\n",
            "  (99966, 91399)\t1\n",
            "  (99966, 57078)\t3\n",
            "  (99966, 13019)\t1\n",
            "  (99966, 69854)\t3\n",
            "  (99966, 30682)\t1\n",
            "  (99966, 86187)\t1\n",
            "  (99966, 88253)\t1\n",
            "  (99966, 18484)\t1\n",
            "  (99966, 83392)\t1\n",
            "  (99966, 29589)\t1\n",
            "  (99966, 84674)\t1\n",
            "  (99966, 87056)\t1\n",
            "  (99966, 101062)\t1\n",
            "  (99966, 20529)\t1\n",
            "  (99966, 61435)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "kBs05MmxnX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "t7OLXsEjnX_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "UD5qEBRYnX_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "qQT61JCknX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EhX6QNnKnX_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "2qNe30adnX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8d32b9-2d91-4add-e732-57fc1674cf3a",
        "id": "2c6rMorSnX_Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "MDi6DqGDnX_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d986a5-83ef-4556-f39f-9142198e24b1",
        "id": "FbzFISEJnX_R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "WCHapHyRnX_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "dPvg0uBmnX_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "aJ9a0o6RnX_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "gDm9EkS4nX_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "yBM11LRmFGsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "YK8YHAtzbo4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXB2puo6brzV",
        "outputId": "69585336-8ef1-4f6a-c288-c9fbf4b9e578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "100067\n",
            "  (0, 95281)\t1\n",
            "  (0, 71860)\t2\n",
            "  (0, 80667)\t2\n",
            "  (0, 92079)\t1\n",
            "  (0, 72004)\t1\n",
            "  (0, 77629)\t2\n",
            "  (0, 27765)\t1\n",
            "  (0, 34027)\t1\n",
            "  (0, 42521)\t1\n",
            "  (0, 47880)\t1\n",
            "  (0, 72033)\t1\n",
            "  (0, 30537)\t1\n",
            "  (0, 31375)\t1\n",
            "  (0, 41255)\t1\n",
            "  (0, 9045)\t1\n",
            "  (0, 27646)\t1\n",
            "  (0, 41375)\t1\n",
            "  (0, 82554)\t1\n",
            "  (0, 24957)\t1\n",
            "  (0, 38676)\t1\n",
            "  (0, 52008)\t1\n",
            "  (0, 50629)\t1\n",
            "  (0, 33833)\t1\n",
            "  (0, 92695)\t1\n",
            "  (1, 27692)\t2\n",
            "  :\t:\n",
            "  (99965, 54025)\t1\n",
            "  (99965, 54952)\t1\n",
            "  (99965, 98921)\t1\n",
            "  (99965, 39689)\t1\n",
            "  (99965, 21390)\t1\n",
            "  (99965, 15050)\t1\n",
            "  (99965, 67281)\t1\n",
            "  (99965, 91339)\t1\n",
            "  (99965, 89745)\t1\n",
            "  (99965, 34014)\t1\n",
            "  (99965, 28036)\t1\n",
            "  (99966, 77632)\t1\n",
            "  (99966, 52623)\t1\n",
            "  (99966, 11586)\t1\n",
            "  (99966, 38485)\t1\n",
            "  (99966, 9758)\t1\n",
            "  (99966, 96617)\t2\n",
            "  (99966, 66456)\t1\n",
            "  (99966, 81562)\t1\n",
            "  (99966, 94365)\t1\n",
            "  (99966, 58148)\t1\n",
            "  (99966, 22165)\t1\n",
            "  (99966, 29109)\t1\n",
            "  (99966, 12503)\t3\n",
            "  (99966, 1351)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "G3UbE_-acYVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "lL7Da5TBcX7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "hiUkrBLHgGow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "CRuIwDBsniJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IX1lsy0mnfD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "4mWT90Wl2SH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRsOhYcQ2RIf",
        "outputId": "97e8b844-2b32-4aaf-b807-1f16f3015578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "hXUjUyy2IETR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAQG3CQB6vKL",
        "outputId": "dfcbc842-8d7f-420e-9ee5-a22969d86480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "UPS7rvoC9N2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "qvKOaHiNiueS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "T-UWxyWF9he2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "0r-afF1V9mT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "XNLsuxcsnKy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "wb4VRwVynKy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1695b94-6a11-40a0-9493-3f40cde6e97d",
        "id": "Kfyv-HPanKy8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "102339\n",
            "  (0, 64201)\t1\n",
            "  (0, 69061)\t1\n",
            "  (0, 34733)\t1\n",
            "  (0, 40821)\t1\n",
            "  (0, 86431)\t1\n",
            "  (0, 41384)\t1\n",
            "  (0, 74620)\t1\n",
            "  (0, 78003)\t1\n",
            "  (0, 28435)\t1\n",
            "  (1, 40821)\t1\n",
            "  (1, 100470)\t2\n",
            "  (1, 50264)\t3\n",
            "  (1, 12213)\t1\n",
            "  (1, 53018)\t1\n",
            "  (1, 87002)\t2\n",
            "  (1, 42628)\t2\n",
            "  (1, 77770)\t1\n",
            "  (1, 67002)\t1\n",
            "  (1, 207)\t1\n",
            "  (1, 100623)\t1\n",
            "  (1, 38613)\t1\n",
            "  (1, 49687)\t1\n",
            "  (1, 97654)\t1\n",
            "  (1, 79503)\t1\n",
            "  (1, 12888)\t1\n",
            "  :\t:\n",
            "  (99964, 21391)\t1\n",
            "  (99965, 69061)\t1\n",
            "  (99965, 55217)\t1\n",
            "  (99965, 100668)\t1\n",
            "  (99965, 66215)\t1\n",
            "  (99965, 69365)\t1\n",
            "  (99965, 80221)\t1\n",
            "  (99965, 51671)\t1\n",
            "  (99965, 84949)\t1\n",
            "  (99965, 99912)\t1\n",
            "  (99966, 66039)\t1\n",
            "  (99966, 89542)\t2\n",
            "  (99966, 41058)\t1\n",
            "  (99966, 95900)\t1\n",
            "  (99966, 62789)\t1\n",
            "  (99966, 47232)\t1\n",
            "  (99966, 27058)\t1\n",
            "  (99966, 55160)\t1\n",
            "  (99966, 14824)\t1\n",
            "  (99966, 19704)\t1\n",
            "  (99966, 60480)\t2\n",
            "  (99966, 14382)\t1\n",
            "  (99966, 33130)\t1\n",
            "  (99966, 50018)\t1\n",
            "  (99966, 83657)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "e4CpellknKy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "cibWhmCznKy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "6Y6G4q0JnKy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "eIGfS7qDnKy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FxS4dKPfnKy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "JmxOEtoCnKy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b44468-aa8e-47e0-f09c-37e4e794cfcc",
        "id": "ffqlVL8pnKy9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "cmpoZLPznKy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18fbb16-9927-48a5-b97a-ac1204def144",
        "id": "Xg2J7F4qnKy-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "x6JU6oYrnKy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "N7iroO9lnKy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "9siY5oAhnKy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "r41OBCgGnKy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "zQ2GYgr3nUAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "1wiCoOVDnUAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378963ff-7895-4294-9f7f-513167974684",
        "id": "uL5AGGfhnUAt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "101482\n",
            "  (0, 74366)\t1\n",
            "  (0, 82756)\t1\n",
            "  (0, 41274)\t1\n",
            "  (0, 16006)\t1\n",
            "  (0, 78296)\t1\n",
            "  (0, 68889)\t1\n",
            "  (0, 27125)\t1\n",
            "  (0, 2926)\t1\n",
            "  (0, 20205)\t1\n",
            "  (0, 37253)\t1\n",
            "  (0, 37264)\t16\n",
            "  (1, 23007)\t1\n",
            "  (1, 25273)\t1\n",
            "  (1, 86987)\t1\n",
            "  (1, 68193)\t1\n",
            "  (2, 87311)\t1\n",
            "  (2, 26719)\t1\n",
            "  (2, 69915)\t1\n",
            "  (2, 12552)\t1\n",
            "  (2, 95116)\t1\n",
            "  (2, 42970)\t1\n",
            "  (2, 55118)\t1\n",
            "  (2, 13702)\t1\n",
            "  (3, 42970)\t1\n",
            "  (3, 68596)\t1\n",
            "  :\t:\n",
            "  (99965, 26500)\t1\n",
            "  (99965, 35895)\t1\n",
            "  (99965, 36600)\t1\n",
            "  (99965, 54485)\t1\n",
            "  (99965, 79262)\t1\n",
            "  (99965, 72248)\t1\n",
            "  (99965, 19895)\t1\n",
            "  (99965, 90787)\t1\n",
            "  (99965, 16879)\t1\n",
            "  (99965, 83627)\t1\n",
            "  (99965, 10793)\t1\n",
            "  (99965, 79644)\t1\n",
            "  (99965, 94160)\t1\n",
            "  (99965, 74826)\t1\n",
            "  (99965, 20123)\t1\n",
            "  (99965, 92531)\t1\n",
            "  (99965, 49573)\t1\n",
            "  (99965, 19064)\t1\n",
            "  (99965, 89052)\t1\n",
            "  (99966, 41921)\t1\n",
            "  (99966, 70817)\t1\n",
            "  (99966, 75054)\t1\n",
            "  (99966, 15646)\t1\n",
            "  (99966, 33203)\t1\n",
            "  (99966, 82022)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "oi-lUzM-nUAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "dR39ECJ9nUAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "ZF3FEwa4nUAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "2N-dpgn4nUAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R485sevTnUAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "e4gPmpwgnUAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000267cb-cf0c-4978-c0ea-af9627ca63c3",
        "id": "pqOKAlZfnUAu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "TF291RpinUAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41bba1e-d83b-4ede-ef7c-ba0623a76f65",
        "id": "ZQvjb-YmnUAu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "iLvDcOSmnUAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "tHog3RLHnUAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "372lq9qvnUAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "7jrt97hWnUAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "YAAcF3LRnUAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "JIG6i61fnUAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a248e1-7af4-4086-a748-963da045927a",
        "id": "UczuRH9hnUAv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "96699\n",
            "  (0, 7145)\t1\n",
            "  (0, 65348)\t4\n",
            "  (0, 69847)\t1\n",
            "  (0, 79030)\t1\n",
            "  (0, 37367)\t1\n",
            "  (0, 90472)\t1\n",
            "  (0, 15794)\t1\n",
            "  (0, 83742)\t1\n",
            "  (1, 73742)\t2\n",
            "  (1, 64302)\t2\n",
            "  (1, 39500)\t1\n",
            "  (1, 87216)\t1\n",
            "  (1, 38715)\t1\n",
            "  (1, 34030)\t1\n",
            "  (1, 56550)\t1\n",
            "  (1, 7684)\t1\n",
            "  (1, 13725)\t1\n",
            "  (1, 70595)\t2\n",
            "  (1, 70524)\t1\n",
            "  (1, 42105)\t4\n",
            "  (1, 6810)\t1\n",
            "  (1, 43901)\t2\n",
            "  (1, 29174)\t1\n",
            "  (1, 11282)\t1\n",
            "  (1, 44624)\t1\n",
            "  :\t:\n",
            "  (99966, 76393)\t1\n",
            "  (99966, 85472)\t1\n",
            "  (99966, 67118)\t1\n",
            "  (99966, 71047)\t3\n",
            "  (99966, 67390)\t1\n",
            "  (99966, 54139)\t1\n",
            "  (99966, 55140)\t2\n",
            "  (99966, 13598)\t2\n",
            "  (99966, 2155)\t1\n",
            "  (99966, 63131)\t1\n",
            "  (99966, 54623)\t1\n",
            "  (99966, 45113)\t5\n",
            "  (99966, 10277)\t1\n",
            "  (99966, 46857)\t1\n",
            "  (99966, 56994)\t14\n",
            "  (99966, 54522)\t1\n",
            "  (99966, 86334)\t1\n",
            "  (99966, 45114)\t1\n",
            "  (99966, 84453)\t1\n",
            "  (99966, 86491)\t1\n",
            "  (99966, 52020)\t1\n",
            "  (99966, 13390)\t1\n",
            "  (99966, 60317)\t1\n",
            "  (99966, 64065)\t1\n",
            "  (99966, 59069)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "dtdLbbFknUAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "u1VgbgRfnUAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "_0erh1QsnUAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "-dwOXoFbnUAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yXm6JSjXnUAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "NV4COhbMnUAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa838db5-872c-48b5-f048-c6982e2b44c0",
        "id": "XQ8eHQJinUAw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "6tvrWycInUAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4801c19-0cbc-4522-a700-e548077d481e",
        "id": "h6ST4Au_nUAw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "Q72_FdLwnUAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "F9sDrBoDnUAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "1uAAibDxnUAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "3KKza0CtnUAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "WPdwFdyxnn4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "3gv3yxgVnn4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfefa155-c06b-48b4-99db-e7e3cae28fb9",
        "id": "kNSvcKPRnn5A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "101855\n",
            "  (0, 36307)\t3\n",
            "  (0, 31725)\t1\n",
            "  (0, 57226)\t1\n",
            "  (0, 30008)\t1\n",
            "  (0, 56282)\t1\n",
            "  (0, 10606)\t1\n",
            "  (0, 88897)\t1\n",
            "  (0, 89188)\t1\n",
            "  (0, 77112)\t1\n",
            "  (0, 99966)\t3\n",
            "  (0, 17581)\t1\n",
            "  (0, 85537)\t1\n",
            "  (0, 73702)\t1\n",
            "  (0, 71843)\t1\n",
            "  (0, 62653)\t2\n",
            "  (0, 16061)\t1\n",
            "  (0, 29204)\t1\n",
            "  (0, 13336)\t2\n",
            "  (0, 30211)\t1\n",
            "  (0, 10455)\t1\n",
            "  (0, 49007)\t1\n",
            "  (0, 88378)\t1\n",
            "  (0, 81849)\t4\n",
            "  (0, 68935)\t1\n",
            "  (0, 90140)\t3\n",
            "  :\t:\n",
            "  (99966, 90140)\t1\n",
            "  (99966, 45272)\t1\n",
            "  (99966, 13669)\t2\n",
            "  (99966, 55184)\t3\n",
            "  (99966, 33228)\t2\n",
            "  (99966, 12053)\t1\n",
            "  (99966, 89039)\t1\n",
            "  (99966, 79121)\t1\n",
            "  (99966, 30660)\t3\n",
            "  (99966, 82850)\t1\n",
            "  (99966, 39152)\t1\n",
            "  (99966, 54348)\t1\n",
            "  (99966, 97483)\t1\n",
            "  (99966, 57615)\t1\n",
            "  (99966, 36871)\t3\n",
            "  (99966, 68743)\t1\n",
            "  (99966, 56499)\t2\n",
            "  (99966, 17221)\t1\n",
            "  (99966, 36139)\t1\n",
            "  (99966, 11087)\t1\n",
            "  (99966, 50371)\t1\n",
            "  (99966, 101313)\t1\n",
            "  (99966, 31734)\t1\n",
            "  (99966, 46535)\t1\n",
            "  (99966, 10095)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "33aiSoCKnn5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "eJ2ocBLTnn5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "fyfC6oHGnn5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "iWNM0VRCnn5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQ44l8bMnn5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "IE-vnRkenn5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460578de-26f2-49eb-ce5e-7f62cac11b6e",
        "id": "14YUHiGBnn5B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "jsTn-wqFnn5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0348b9ed-ca5d-4550-ac71-297405c4a660",
        "id": "Cv0cFlOInn5B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "Eh3RIYt4nn5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "bm7iypVKnn5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "-8sWNUmlnn5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "0XTsKLiMnn5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "2hY2zWTdnn5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "WJq9kfrvnn5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333558e5-3c52-408c-aa02-36602b402bd2",
        "id": "tlVjsBfynn5C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "101201\n",
            "  (0, 100547)\t1\n",
            "  (0, 43218)\t1\n",
            "  (0, 96862)\t1\n",
            "  (0, 83444)\t1\n",
            "  (0, 61000)\t2\n",
            "  (0, 91468)\t1\n",
            "  (0, 41372)\t1\n",
            "  (0, 81439)\t1\n",
            "  (0, 77785)\t1\n",
            "  (0, 44010)\t1\n",
            "  (0, 88599)\t1\n",
            "  (0, 96375)\t1\n",
            "  (0, 73944)\t1\n",
            "  (0, 41738)\t1\n",
            "  (0, 26906)\t1\n",
            "  (0, 78049)\t1\n",
            "  (1, 41372)\t1\n",
            "  (1, 73944)\t1\n",
            "  (1, 92724)\t1\n",
            "  (1, 9804)\t1\n",
            "  (1, 78678)\t1\n",
            "  (1, 41013)\t1\n",
            "  (1, 2914)\t2\n",
            "  (1, 97230)\t2\n",
            "  (1, 26774)\t1\n",
            "  :\t:\n",
            "  (99965, 75090)\t1\n",
            "  (99965, 86425)\t1\n",
            "  (99965, 36436)\t1\n",
            "  (99965, 48069)\t1\n",
            "  (99965, 73647)\t2\n",
            "  (99966, 43218)\t1\n",
            "  (99966, 91468)\t1\n",
            "  (99966, 41372)\t1\n",
            "  (99966, 52976)\t1\n",
            "  (99966, 78138)\t1\n",
            "  (99966, 65503)\t1\n",
            "  (99966, 22050)\t1\n",
            "  (99966, 89589)\t1\n",
            "  (99966, 25310)\t1\n",
            "  (99966, 16066)\t1\n",
            "  (99966, 47163)\t1\n",
            "  (99966, 92065)\t2\n",
            "  (99966, 25393)\t1\n",
            "  (99966, 31864)\t1\n",
            "  (99966, 14952)\t1\n",
            "  (99966, 32261)\t1\n",
            "  (99966, 89786)\t1\n",
            "  (99966, 59299)\t1\n",
            "  (99966, 1079)\t1\n",
            "  (99966, 1318)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "YPA2ULU9nn5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "4lKbgAgXnn5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "P3BBrLrFnn5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "YexfPGYAnn5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nrHyU5Y4nn5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "8t4zBNEhnn5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc52d9f3-6254-46d2-c99b-2446974861dd",
        "id": "FUrBP9FVnn5D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "HA7NxGbUnn5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589a7215-34e4-4cd0-e431-f5e448b9397b",
        "id": "c3XtqSn7nn5D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "rRPucVrxnn5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "RdSRzXq3nn5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "FpR3hI8Unn5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "VuxRvG93nn5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "99LaieNznn5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "GXvkOpNOnn5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a96bb1-a458-489c-ca0b-fe655bf86706",
        "id": "rSzYYlE6nn5F"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "99362\n",
            "  (0, 34598)\t1\n",
            "  (0, 38316)\t1\n",
            "  (0, 46847)\t1\n",
            "  (0, 82628)\t1\n",
            "  (0, 56196)\t1\n",
            "  (0, 42006)\t1\n",
            "  (0, 78644)\t2\n",
            "  (0, 94385)\t9\n",
            "  (0, 1330)\t1\n",
            "  (0, 98243)\t1\n",
            "  (0, 62924)\t1\n",
            "  (0, 72832)\t1\n",
            "  (0, 56131)\t1\n",
            "  (0, 6653)\t2\n",
            "  (0, 16662)\t1\n",
            "  (0, 17775)\t1\n",
            "  (0, 52272)\t1\n",
            "  (0, 84724)\t3\n",
            "  (0, 57746)\t1\n",
            "  (0, 88381)\t1\n",
            "  (0, 23815)\t1\n",
            "  (0, 86446)\t1\n",
            "  (0, 61533)\t1\n",
            "  (0, 81159)\t1\n",
            "  (0, 63087)\t2\n",
            "  :\t:\n",
            "  (99965, 9723)\t1\n",
            "  (99965, 20871)\t1\n",
            "  (99965, 70493)\t1\n",
            "  (99965, 38572)\t1\n",
            "  (99966, 38499)\t1\n",
            "  (99966, 39151)\t1\n",
            "  (99966, 84652)\t1\n",
            "  (99966, 86479)\t1\n",
            "  (99966, 77807)\t1\n",
            "  (99966, 36627)\t1\n",
            "  (99966, 50050)\t1\n",
            "  (99966, 71697)\t1\n",
            "  (99966, 68405)\t1\n",
            "  (99966, 69192)\t1\n",
            "  (99966, 10263)\t1\n",
            "  (99966, 92865)\t1\n",
            "  (99966, 53713)\t1\n",
            "  (99966, 93630)\t1\n",
            "  (99966, 61034)\t1\n",
            "  (99966, 61836)\t1\n",
            "  (99966, 15812)\t1\n",
            "  (99966, 13203)\t1\n",
            "  (99966, 74014)\t1\n",
            "  (99966, 36101)\t1\n",
            "  (99966, 13274)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "6TLZopTGnn5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "x5-23CFynn5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "5uCRwmt7nn5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "7f5t1hnYnn5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W_EqC2ZFnn5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "ZbZMdcTbnn5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363d2bd9-87e2-4d71-a7ac-828e26b6acd6",
        "id": "9mcCPB9Inn5G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "btKy3jyrnn5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db5e2f3-6b53-47a9-8cf4-6e0a6d3277ed",
        "id": "bD8V5P30nn5G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "B0mOmot8nn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "JF6HA52Unn5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "OhHkdkypnn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "rmh8aOj4nn5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "BHmUQ29Wnn5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "fflRIVcunn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7150760-54ab-4820-9548-f548c0e77870",
        "id": "NdNQXnEPnn5H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "102723\n",
            "  (0, 89989)\t1\n",
            "  (0, 41770)\t2\n",
            "  (0, 54389)\t1\n",
            "  (0, 98691)\t1\n",
            "  (0, 91178)\t1\n",
            "  (0, 19232)\t1\n",
            "  (0, 92839)\t1\n",
            "  (0, 13590)\t1\n",
            "  (1, 41770)\t4\n",
            "  (1, 91178)\t1\n",
            "  (1, 92839)\t1\n",
            "  (1, 37054)\t1\n",
            "  (1, 81886)\t1\n",
            "  (1, 12144)\t2\n",
            "  (1, 35242)\t1\n",
            "  (1, 81887)\t1\n",
            "  (1, 67738)\t1\n",
            "  (1, 37139)\t5\n",
            "  (1, 31828)\t1\n",
            "  (1, 38087)\t1\n",
            "  (1, 78323)\t1\n",
            "  (1, 36211)\t1\n",
            "  (1, 66628)\t2\n",
            "  (1, 69195)\t2\n",
            "  (1, 83651)\t4\n",
            "  :\t:\n",
            "  (99966, 19597)\t2\n",
            "  (99966, 46831)\t1\n",
            "  (99966, 82689)\t1\n",
            "  (99966, 88380)\t1\n",
            "  (99966, 65147)\t1\n",
            "  (99966, 79159)\t1\n",
            "  (99966, 58985)\t1\n",
            "  (99966, 90837)\t1\n",
            "  (99966, 25173)\t1\n",
            "  (99966, 102316)\t1\n",
            "  (99966, 58503)\t1\n",
            "  (99966, 73786)\t1\n",
            "  (99966, 64331)\t1\n",
            "  (99966, 72772)\t1\n",
            "  (99966, 43932)\t1\n",
            "  (99966, 91892)\t1\n",
            "  (99966, 22136)\t1\n",
            "  (99966, 46819)\t1\n",
            "  (99966, 48258)\t1\n",
            "  (99966, 65699)\t1\n",
            "  (99966, 24554)\t1\n",
            "  (99966, 23818)\t1\n",
            "  (99966, 70588)\t1\n",
            "  (99966, 59736)\t1\n",
            "  (99966, 93335)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "HCglcVfnnn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "4YsHu7e0nn5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "TBqAzxpUnn5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "FCTZfZqAnn5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KRS_dY3xnn5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "StOYLp9-nn5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c87bb2-5ec3-4c74-e8f3-97c32aa72baa",
        "id": "L6h87rSznn5I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "IU9KAZR7nn5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b5587b-9813-435d-afa7-a6d78aecca2c",
        "id": "feH412HVnn5I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "SCvguyxwnn5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "pUX0dZ0Mnn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "rZX-V2E-nn5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "8ZLSFS0knn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "6itkjTExnn5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "Mba6bE0Enn5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c6b88b-593d-4677-fc7e-7454f192d365",
        "id": "umL8X55Dnn5K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "98044\n",
            "  (0, 74507)\t1\n",
            "  (0, 85411)\t1\n",
            "  (0, 86489)\t1\n",
            "  (0, 7837)\t1\n",
            "  (0, 93490)\t2\n",
            "  (0, 37479)\t1\n",
            "  (0, 39934)\t3\n",
            "  (0, 62706)\t2\n",
            "  (0, 93234)\t3\n",
            "  (0, 28635)\t1\n",
            "  (0, 62270)\t1\n",
            "  (0, 74527)\t3\n",
            "  (0, 67967)\t2\n",
            "  (0, 23881)\t1\n",
            "  (0, 69913)\t2\n",
            "  (0, 51609)\t1\n",
            "  (0, 11985)\t1\n",
            "  (0, 18519)\t1\n",
            "  (0, 69877)\t1\n",
            "  (0, 13575)\t1\n",
            "  (0, 38059)\t1\n",
            "  (0, 12254)\t1\n",
            "  (0, 70736)\t1\n",
            "  (0, 49691)\t1\n",
            "  (0, 65255)\t1\n",
            "  :\t:\n",
            "  (99965, 56529)\t1\n",
            "  (99965, 27276)\t1\n",
            "  (99965, 79621)\t1\n",
            "  (99965, 30294)\t1\n",
            "  (99965, 94742)\t1\n",
            "  (99965, 83011)\t1\n",
            "  (99965, 87234)\t1\n",
            "  (99965, 77680)\t1\n",
            "  (99965, 31740)\t2\n",
            "  (99965, 6703)\t1\n",
            "  (99965, 424)\t1\n",
            "  (99965, 79365)\t1\n",
            "  (99965, 76561)\t1\n",
            "  (99965, 96979)\t2\n",
            "  (99965, 73337)\t1\n",
            "  (99965, 79992)\t1\n",
            "  (99966, 23881)\t1\n",
            "  (99966, 51609)\t1\n",
            "  (99966, 38059)\t1\n",
            "  (99966, 87319)\t1\n",
            "  (99966, 12627)\t1\n",
            "  (99966, 20525)\t1\n",
            "  (99966, 15323)\t1\n",
            "  (99966, 22779)\t1\n",
            "  (99966, 92191)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "8g_f_mRsnn5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "v8sWiv5Fnn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "N3OOOnqmnn5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "yzIsAeT7nn5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yas5V0_Lnn5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "jixALiVAnn5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a719c5da-3f10-485e-abfc-7143ea735b63",
        "id": "A-9f5b13nn5L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "47pHWrDRnn5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f71ef7e-b680-41a1-e791-357c81e4af2b",
        "id": "PbCtJG0Tnn5L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "68Q6S_uVnn5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "aKhGrpuMnn5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "JJ5MIMrlnn5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "S0XDj_LRnn5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "bplFgdkEnn5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "Vc9LepOknn5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab7ffe6-de3d-47a8-9c41-98e43ec58c77",
        "id": "YzlOfz7Unn5M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "98851\n",
            "  (0, 17163)\t1\n",
            "  (0, 31480)\t1\n",
            "  (0, 62611)\t1\n",
            "  (0, 44015)\t1\n",
            "  (0, 71498)\t1\n",
            "  (0, 86461)\t1\n",
            "  (0, 96984)\t1\n",
            "  (0, 34190)\t1\n",
            "  (0, 65600)\t1\n",
            "  (0, 17809)\t1\n",
            "  (0, 37997)\t1\n",
            "  (0, 40599)\t1\n",
            "  (0, 53329)\t1\n",
            "  (0, 39964)\t2\n",
            "  (0, 87013)\t1\n",
            "  (0, 10532)\t1\n",
            "  (0, 35666)\t1\n",
            "  (0, 32825)\t1\n",
            "  (0, 43617)\t1\n",
            "  (0, 94092)\t1\n",
            "  (0, 36810)\t1\n",
            "  (0, 4265)\t1\n",
            "  (0, 97740)\t1\n",
            "  (0, 23214)\t1\n",
            "  (0, 82656)\t1\n",
            "  :\t:\n",
            "  (99966, 79704)\t1\n",
            "  (99966, 94706)\t1\n",
            "  (99966, 14990)\t2\n",
            "  (99966, 81447)\t1\n",
            "  (99966, 83265)\t1\n",
            "  (99966, 87584)\t1\n",
            "  (99966, 89494)\t1\n",
            "  (99966, 17377)\t1\n",
            "  (99966, 24766)\t1\n",
            "  (99966, 82197)\t1\n",
            "  (99966, 96672)\t2\n",
            "  (99966, 62308)\t1\n",
            "  (99966, 57848)\t1\n",
            "  (99966, 80493)\t1\n",
            "  (99966, 43495)\t1\n",
            "  (99966, 58544)\t1\n",
            "  (99966, 20563)\t1\n",
            "  (99966, 43465)\t1\n",
            "  (99966, 78232)\t1\n",
            "  (99966, 40881)\t2\n",
            "  (99966, 78879)\t1\n",
            "  (99966, 89670)\t1\n",
            "  (99966, 50213)\t1\n",
            "  (99966, 80984)\t1\n",
            "  (99966, 55147)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "FqSe-whRnn5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "cj10Jt6dnn5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "6_0lAc-Znn5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "B8PSNKcann5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8EQLcpz3nn5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "p4v0_3A_nn5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57297da5-d6bf-42cb-9e1a-b6001ea3c100",
        "id": "NgNMBR6znn5N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "bVa-2mg1nn5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d064ace6-9ee3-4151-fd33-2370d119bbdf",
        "id": "D77nrujAnn5N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "Jov9Hovdnn5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "wq_BS2nFnn5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "e6aqB_0jnn5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "Vcz5KT0inn5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "scRQ8Rkann5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "IfKu7ALsnn5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b74413-e5a0-432d-e961-0ecb80211354",
        "id": "eivUX436nn5O"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "98625\n",
            "  (0, 93885)\t1\n",
            "  (0, 43135)\t1\n",
            "  (0, 41627)\t1\n",
            "  (0, 79573)\t1\n",
            "  (0, 22778)\t1\n",
            "  (0, 59598)\t1\n",
            "  (0, 6293)\t1\n",
            "  (0, 29546)\t1\n",
            "  (0, 60872)\t2\n",
            "  (0, 37284)\t2\n",
            "  (0, 14200)\t2\n",
            "  (0, 6442)\t1\n",
            "  (1, 50902)\t1\n",
            "  (1, 15027)\t1\n",
            "  (1, 87225)\t1\n",
            "  (1, 75808)\t1\n",
            "  (1, 77197)\t1\n",
            "  (1, 85845)\t1\n",
            "  (1, 96446)\t1\n",
            "  (1, 39957)\t1\n",
            "  (1, 13587)\t1\n",
            "  (2, 85996)\t1\n",
            "  (2, 20774)\t1\n",
            "  (2, 71065)\t1\n",
            "  (2, 52121)\t1\n",
            "  :\t:\n",
            "  (99966, 58047)\t2\n",
            "  (99966, 20506)\t1\n",
            "  (99966, 310)\t2\n",
            "  (99966, 38268)\t4\n",
            "  (99966, 62964)\t1\n",
            "  (99966, 54911)\t1\n",
            "  (99966, 77552)\t1\n",
            "  (99966, 38880)\t1\n",
            "  (99966, 22034)\t1\n",
            "  (99966, 22953)\t1\n",
            "  (99966, 29152)\t1\n",
            "  (99966, 96897)\t1\n",
            "  (99966, 76223)\t1\n",
            "  (99966, 68985)\t1\n",
            "  (99966, 29153)\t10\n",
            "  (99966, 67504)\t2\n",
            "  (99966, 7343)\t1\n",
            "  (99966, 30594)\t1\n",
            "  (99966, 88460)\t3\n",
            "  (99966, 77586)\t1\n",
            "  (99966, 64380)\t8\n",
            "  (99966, 30938)\t1\n",
            "  (99966, 64381)\t7\n",
            "  (99966, 23062)\t2\n",
            "  (99966, 47447)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "kKVRJgZ2nn5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "RY8n0ujLnn5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "mfggnOIunn5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "amXiyiWMnn5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4XUFvX8Vnn5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "Z6IDEc0snn5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27918e2-c31d-4be2-e81a-ddb927eae889",
        "id": "xxkU-mmrnn5P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "SfWf67SDnn5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2f326e-0b28-4f24-d6f0-dfeab82bb4e7",
        "id": "Nxryehlonn5P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "Th0n2XOGnn5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "Fj3M3fdEnn5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "gfPz3RQtnn5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "fJ3bo3Hfnn5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mult *Run*"
      ],
      "metadata": {
        "id": "eIILEEhPnn5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split for Train"
      ],
      "metadata": {
        "id": "sj0-18Xann5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, test_data = train_test_split(data, train_size = 0.6)\n",
        "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False) \n",
        "#Adjust for ngrams later\n",
        "print(training_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_bow = bow_transform.fit_transform(training_data['clean'])\n",
        "print(len(bow_transform.vocabulary_))\n",
        "print(x_train_bow)\n",
        "\n",
        "X_te_bow = bow_transform.transform(test_data['clean'])\n",
        "\n",
        "y_train = training_data['label']\n",
        "y_train=y_train.astype('int')\n",
        "y_test = test_data['label']\n",
        "y_test=y_test.astype('int')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b52f6e97-2f5e-4f31-f7e1-f3674f70abac",
        "id": "iuohLoqInn5Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99967, 3)\n",
            "(66645, 3)\n",
            "102935\n",
            "  (0, 74892)\t2\n",
            "  (0, 32452)\t2\n",
            "  (0, 47173)\t2\n",
            "  (0, 68702)\t2\n",
            "  (0, 57064)\t1\n",
            "  (0, 83966)\t3\n",
            "  (0, 87222)\t1\n",
            "  (0, 57087)\t2\n",
            "  (0, 86965)\t2\n",
            "  (0, 66480)\t1\n",
            "  (0, 28048)\t1\n",
            "  (0, 39468)\t1\n",
            "  (0, 46599)\t1\n",
            "  (0, 79384)\t1\n",
            "  (0, 32712)\t1\n",
            "  (0, 90105)\t3\n",
            "  (0, 84892)\t1\n",
            "  (0, 31959)\t1\n",
            "  (0, 14008)\t1\n",
            "  (0, 101787)\t2\n",
            "  (0, 80553)\t1\n",
            "  (0, 21380)\t2\n",
            "  (0, 66437)\t1\n",
            "  (0, 45398)\t1\n",
            "  (0, 78416)\t1\n",
            "  :\t:\n",
            "  (99965, 23080)\t1\n",
            "  (99965, 61226)\t1\n",
            "  (99965, 45082)\t1\n",
            "  (99965, 67121)\t1\n",
            "  (99965, 25018)\t1\n",
            "  (99965, 20405)\t2\n",
            "  (99965, 54311)\t2\n",
            "  (99965, 96211)\t1\n",
            "  (99965, 76619)\t1\n",
            "  (99965, 87687)\t1\n",
            "  (99965, 66079)\t1\n",
            "  (99965, 63958)\t1\n",
            "  (99965, 83282)\t1\n",
            "  (99965, 16758)\t1\n",
            "  (99965, 48421)\t1\n",
            "  (99965, 11047)\t1\n",
            "  (99965, 31431)\t1\n",
            "  (99965, 40355)\t1\n",
            "  (99965, 85624)\t1\n",
            "  (99965, 100598)\t1\n",
            "  (99965, 46741)\t1\n",
            "  (99965, 44546)\t1\n",
            "  (99966, 36885)\t1\n",
            "  (99966, 18962)\t1\n",
            "  (99966, 94201)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "qOSQESubnn5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transform = TfidfTransformer(norm=None)\n",
        "X_tr_tfidf = tfidf_transform.fit_transform(x_train_bow)\n",
        "# print(X_tr_tfidf)\n",
        "X_te_tfidf = tfidf_transform.transform(X_te_bow)\n"
      ],
      "metadata": {
        "id": "hYfI__3Ynn5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "uzQ7-B1wnn5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "5lOhfU1Nnn5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logRegFunction(x_train, y_train, x_test, y_test, info):\n",
        "  logReg = LogisticRegression(max_iter=100)\n",
        "  logReg.fit(x_train, y_train)\n",
        "  y_predicted_prob = logReg.predict_proba(x_test)\n",
        "  y_predicted = logReg.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def mnNaiveBayesFunction(x_train, y_train, x_test, y_test, info):\n",
        "  classifier = naive_bayes.MultinomialNB()\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_predicted = classifier.predict(x_test)\n",
        "  return (info,) + metricsCalc(y_test, y_predicted)\n",
        "\n",
        "def metricsCalc(y_test, y_predicted):\n",
        "  #acc score\n",
        "  accuracy = accuracy_score(y_test.tolist(), y_predicted)\n",
        "\n",
        "  #create seperate dataframes\n",
        "  accuracyData = pd.DataFrame(\n",
        "      {'Y' : y_test.tolist(), 'Y Predicted': y_predicted.tolist()} )\n",
        "  GroundTruthSuicidal = accuracyData[accuracyData[\"Y\"] == 1]\n",
        "  GroundTruthNonSuicidal = accuracyData[accuracyData[\"Y\"] == 0]\n",
        "\n",
        "  #True Positive and False Negative Counts\n",
        "  true_positive_count = GroundTruthSuicidal['Y Predicted'].tolist().count(1)\n",
        "  false_negative_count = GroundTruthSuicidal['Y Predicted'].tolist().count(0)\n",
        "\n",
        "  #True Negative and False Positive Counts\n",
        "  true_negative_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(0)\n",
        "  false_positive_count = GroundTruthNonSuicidal['Y Predicted'].tolist().count(1)\n",
        "\n",
        "  #True Positive, True Negative, Precision \n",
        "  true_positive_rate = accuracy_score(GroundTruthSuicidal['Y'], GroundTruthSuicidal['Y Predicted'])\n",
        "  true_negative_rate = accuracy_score(GroundTruthNonSuicidal['Y'], GroundTruthNonSuicidal['Y Predicted'])\n",
        "  positive_precision = true_positive_count/(true_positive_count + false_positive_count)\n",
        "  f1 = f1_score(y_test.tolist(), y_predicted.tolist())\n",
        "  return accuracy, true_positive_rate, true_negative_rate, positive_precision, f1\n",
        "\n",
        "def addDataToFile(results):\n",
        "  file1 = open(\"/content/drive/MyDrive/Machine Learning/Data/results.csv\", \"a+\")\n",
        "  file1.write((','.join(str(item) for item in results)) + \"\\n\")\n",
        "  file1.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AHZHuxe7nn5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Logistic Regression"
      ],
      "metadata": {
        "id": "_FdTfhEEnn5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_bow_log = logRegFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Logistic Regression\")\n",
        "addDataToFile(results_bow_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af6affd-1956-4bd0-d837-c4dc771301d5",
        "id": "la2Ubw-Lnn5R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDIDF Log Reg"
      ],
      "metadata": {
        "id": "JXMNNGBhnn5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_log = logRegFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TDIDF Logistic Regression\")\n",
        "addDataToFile(results_TDIF_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0033c716-6861-4050-a466-83da057ca8dc",
        "id": "cvQAXBR0nn5R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "s4KH9qx9nn5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_BOW_mnb = mnNaiveBayesFunction(x_train_bow, y_train, X_te_bow, y_test, \"Bag of Words Multinomial Naive Bayes\")\n",
        "addDataToFile(results_BOW_mnb)"
      ],
      "metadata": {
        "id": "aE2JHvjgnn5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFIDF Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "0iQrhgnGnn5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_TDIF_mnb = mnNaiveBayesFunction(X_tr_tfidf, y_train, X_te_tfidf, y_test, \"TFIDF Multinomial Naive Bayes\")\n",
        "addDataToFile(results_TDIF_mnb)"
      ],
      "metadata": {
        "id": "h73deSIxnn5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}